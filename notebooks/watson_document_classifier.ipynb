{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification and Attribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services.\n",
    "\n",
    "### 1.1 Install the necessary packages\n",
    "\n",
    "You need the latest versions of these packages:<br>\n",
    "Watson Developer Cloud: a client library for Watson services.<br>\n",
    "NLTK: leading platform for building Python programs to work with human language data.<br>\n",
    "python-keystoneclient: is a client for the OpenStack Identity API.<br>\n",
    "python-swiftclient: is a python client for the Swift API.<br><br>\n",
    "** Install the Watson Developer Cloud package: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade watson-developer-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install NLTK: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install IBM Bluemix Object Storage Client: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install python-keystoneclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install python-swiftclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** <font color=blue>Now restart the kernel by choosing Kernel > Restart. </font> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import packages and libraries\n",
    "\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import thread\n",
    "import time\n",
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 \\\n",
    "  as Features\n",
    "    \n",
    "import swiftclient\n",
    "from keystoneclient import client\n",
    "    \n",
    "import operator\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from os.path import join, dirname\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize,ne_chunk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Add configurable items of the notebook below\n",
    "\n",
    "### 2.1 Add your service credentials from Bluemix for the Watson services\n",
    "\n",
    "You must create a Watson Natural Language Understanding service on Bluemix.\n",
    "Create a service for Natural Language Understanding (NLU).\n",
    "Insert the username and password values for your NLU in the following cell. Do not change the values of the version fields.\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2017-02-27',\n",
    "    username='',\n",
    "    password='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Add your service credentials for Object Storage\n",
    "\n",
    "You must create Object Storage service on Bluemix.\n",
    "To access data in a file in Object Storage, you need the Object Storage authentication credentials.\n",
    "Insert the Object Storage authentication credentials in the following cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "credentials_1 = {\n",
    "  'auth_url':'',\n",
    "  'project':'',\n",
    "  'project_id':'',\n",
    "  'region':'',\n",
    "  'user_id':'',\n",
    "  'domain_id':'',\n",
    "  'domain_name':'',\n",
    "  'username':'',\n",
    "  'password':\"\"\"\"\"\",\n",
    "  'container':'',\n",
    "  'tenantId':'',\n",
    "  'filename':''\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Global Variables\n",
    "\n",
    "Add global variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify file names for sample text and configuration files\n",
    "sampleTextFileName = \"sample_text.txt\"\n",
    "sampleConfigFileName = \"sample_config.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Configure and download required NLTK packages\n",
    "\n",
    "Download the 'punkt' and 'averaged_perceptron_tagger' NLTK packages for POS tagging usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification\n",
    "\n",
    "Write the classification related utility functions in a modularalized form.\n",
    "\n",
    "### 3.1 Watson NLU Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_using_NLU(analysistext):\n",
    "    response = natural_language_understanding.analyze( \n",
    "        text=analysistext,features=[ Features.Entities(\n",
    "                                        emotion=True,\n",
    "                                        sentiment=True,\n",
    "                                        limit=2\n",
    "                                     ),\n",
    "                                     Features.Keywords(\n",
    "                                        emotion=True,\n",
    "                                        sentiment=True,\n",
    "                                        limit=2\n",
    "                                     )\n",
    "                                   ] )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Augumented Classification\n",
    "\n",
    "Custom classification utlity fucntions for augumenting the results of Watson NLU API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Splitting the text into sentences \n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "### Splitting the text into tokens \n",
    "def split_into_tokens(text):\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "### Part of speech tagging of the text \n",
    "def POS_tagging(text):\n",
    "\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "### Tagging of the text matching keywords\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "\n",
    "### Tagging of the text matching REGEX\n",
    "def regex_tagging(tag,regex,text):\n",
    "    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "### Tagging of the text using chunking\n",
    "def chunk_tagging(tag,chunk,text):\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "\n",
    "### Update the NLU response JSON with augumented classifications\n",
    "def augument_NLUResponse(responsejson,updateType,text,tag):\n",
    "\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n",
    "            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['entities']):\n",
    "            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1})        \n",
    "    \n",
    "\n",
    "### Perform augumented classification of the text \n",
    "def classify_text(text):\n",
    "\n",
    "    ### Classification of the text using Watson NLU\n",
    "    response = analyze_using_NLU(text)\n",
    "    responsejson = response\n",
    "    \n",
    "    ### Start performing Augumented Classification steps\n",
    "    \n",
    "    ### Split sentences    \n",
    "    sentenceList = split_sentences(text)\n",
    "    for sentence in sentenceList:\n",
    "        print(\"Sentence:\", sentence)\n",
    "    \n",
    "    ### Spilt into tokens    \n",
    "    tokens = split_into_tokens(text)\n",
    "    \n",
    "    ### Perform POS tagging of tokens    \n",
    "    postags = POS_tagging(tokens)\n",
    "    \n",
    "    ### Lookup the configuration file to perform tagging steps    \n",
    "    configjson = json.loads(config)\n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        print('## Performing ' + stages['name']+' ##')\n",
    "        for steps in stages['steps']:\n",
    "            print('-- Performing Step ' + steps['type']+' --')\n",
    "            if (steps['type'] == 'keywords'):\n",
    "                for keyword in steps['keywords']:\n",
    "                    for word in sentenceList:\n",
    "                        wordtag = keyword_tagging(keyword['tag'],keyword['text'],word)\n",
    "                        if(wordtag != 'UNKNOWN'):\n",
    "                            print('** '+keyword['tag']+':'+wordtag)\n",
    "                            augument_NLUResponse(responsejson,'entities',wordtag,keyword['tag'])\n",
    "            elif(steps['type'] == 'd_regex'):\n",
    "                for regex in steps['d_regex']:\n",
    "                    for word in sentenceList:\n",
    "                        regextags = regex_tagging(regex['tag'],regex['pattern'],word)\n",
    "                        if (len(regextags)>0):\n",
    "                            for words in regextags:\n",
    "                                print('** '+regex['tag']+':'+words)\n",
    "                                augument_NLUResponse(responsejson,'entities',words,regex['tag'])\n",
    "            elif(steps['type'] == 'chunking'):\n",
    "                for chunk in steps['chunk']:\n",
    "                    chunktags = chunk_tagging(chunk['tag'],chunk['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            print('** '+chunk['tag']+':'+words)\n",
    "                            augument_NLUResponse(responsejson,'entities',words,chunk['tag'])\n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    return responsejson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persistence and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 Configure Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_url = credentials_1['auth_url']+\"/v3\"\n",
    "container = credentials_1[\"container\"]\n",
    "\n",
    "IBM_Objectstorage_Connection = swiftclient.Connection(\n",
    "    key=credentials_1['password'], authurl=auth_url, auth_version='3', os_options={\n",
    "        \"project_id\": credentials_1['project_id'], \"user_id\": credentials_1['user_id'], \"region_name\": credentials_1['region']})\n",
    "\n",
    "def create_container(container_name):\n",
    "    x = IBM_Objectstorage_Connection.put_container(container_name)\n",
    "    return x\n",
    "\n",
    "def put_object(container_name, fname, contents, content_type):\n",
    "    x = IBM_Objectstorage_Connection.put_object(\n",
    "        container_name,\n",
    "        fname,\n",
    "        contents,\n",
    "        content_type)\n",
    "    return x\n",
    "\n",
    "def get_object(container_name, fname):\n",
    "    Object_Store_file_details = IBM_Objectstorage_Connection.get_object(\n",
    "        container_name, fname)\n",
    "    return Object_Store_file_details[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classify text\n",
    "Read the data file for classification from Object Store<br>\n",
    "Read the configuration file for augumented classification from Object Store.<br>\n",
    "Persist the classification results as JSON file in object store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = get_object(container, sampleTextFileName)\n",
    "config = get_object(container, sampleConfigFileName)\n",
    "\n",
    "response = str(classify_text(text))\n",
    "\n",
    "put_object(container, \"sample_text_classification.txt\", response, \"text\")\n",
    "get_object(container, \"sample_text_classification.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
