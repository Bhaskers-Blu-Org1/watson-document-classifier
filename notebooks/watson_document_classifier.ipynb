{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification and Attribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services.\n",
    "\n",
    "### 1.1 Install the necessary packages\n",
    "\n",
    "You need the latest versions of these packages:<br>\n",
    "Watson Developer Cloud: a client library for Watson services.<br>\n",
    "NLTK: leading platform for building Python programs to work with human language data.<br>\n",
    "python-keystoneclient: is a client for the OpenStack Identity API.<br>\n",
    "python-swiftclient: is a python client for the Swift API.<br><br>\n",
    "** Install the Watson Developer Cloud package: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: watson-developer-cloud in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages\n",
      "Requirement already up-to-date: pyOpenSSL>=16.2.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from watson-developer-cloud)\n",
      "Requirement already up-to-date: python-dateutil>=2.5.3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from watson-developer-cloud)\n",
      "Requirement already up-to-date: requests<3.0,>=2.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from watson-developer-cloud)\n",
      "Requirement already up-to-date: pysolr<4.0,>=3.3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from watson-developer-cloud)\n",
      "Requirement already up-to-date: cryptography>=1.9 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: six>=1.5.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: cffi>=1.7; platform_python_implementation != \"PyPy\" in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: enum34; python_version < \"3\" in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: asn1crypto>=0.21.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: ipaddress; python_version < \"3\" in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement already up-to-date: pycparser in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from cffi>=1.7; platform_python_implementation != \"PyPy\"->cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade watson-developer-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install NLTK: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages\n",
      "Requirement already up-to-date: six in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install IBM Bluemix Object Storage Client: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-swiftclient in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: requests>=1.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from python-swiftclient)\n",
      "Requirement already satisfied: futures>=3.0; python_version == \"2.7\" or python_version == \"2.6\" in /usr/local/src/bluemix_jupyter_bundle.v70/notebook/lib/python2.7/site-packages (from python-swiftclient)\n",
      "Requirement already satisfied: six>=1.5.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from python-swiftclient)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests>=1.1->python-swiftclient)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests>=1.1->python-swiftclient)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests>=1.1->python-swiftclient)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s973-7d640fb4db0d6f-c5c16a29391b/.local/lib/python2.7/site-packages (from requests>=1.1->python-swiftclient)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-swiftclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** <font color=blue>Now restart the kernel by choosing Kernel > Restart. </font> **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import packages and libraries\n",
    "\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, KeywordsOptions\n",
    "    \n",
    "import swiftclient\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize,ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Add configurable items of the notebook below\n",
    "\n",
    "### 2.1 Add your service credentials from Bluemix for the Watson services\n",
    "\n",
    "You must create a Watson Natural Language Understanding service on Bluemix.\n",
    "Create a service for Natural Language Understanding (NLU).\n",
    "Insert the username and password values for your NLU in the following cell. Do not change the values of the version fields.\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2017-02-27',\n",
    "    username=\"\",\n",
    "    password=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Add your service credentials for Object Storage\n",
    "\n",
    "You must create Object Storage service on Bluemix.\n",
    "To access data in a file in Object Storage, you need the Object Storage authentication credentials.\n",
    "Insert the Object Storage authentication credentials as <i><b>credentials_1</b></i> in the following cell after \n",
    "removing the current contents in the cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "credentials_1 = {\n",
    "  'auth_url':'',\n",
    "  'project':'',\n",
    "  'project_id':'',\n",
    "  'region':'',\n",
    "  'user_id':'',\n",
    "  'domain_id':'',\n",
    "  'domain_name':'',\n",
    "  'username':'',\n",
    "  'password':'',\n",
    "  'container':'',\n",
    "  'tenantId':'',\n",
    "  'filename':''\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Global Variables\n",
    "\n",
    "Add global variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify file names for sample text and configuration files\n",
    "sampleTextFileName = \"sample_text.txt\"\n",
    "sampleConfigFileName = \"sample_config.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Configure and download required NLTK packages\n",
    "\n",
    "Download the 'punkt' and 'averaged_perceptron_tagger' NLTK packages for POS tagging usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /gpfs/fs01/user/s973\n",
      "[nltk_data]     -7d640fb4db0d6f-c5c16a29391b/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /gpfs/fs01/user/s973-7d640fb4db0d6f-\n",
      "[nltk_data]     c5c16a29391b/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification\n",
    "\n",
    "Write the classification related utility functions in a modularized form.\n",
    "\n",
    "### 3.1 Watson NLU Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_using_NLU(analysistext):\n",
    "    \"\"\" Call Watson Natural Language Understanding service to obtain analysis results.\n",
    "    \"\"\"\n",
    "    response = natural_language_understanding.analyze( \n",
    "        text=analysistext,\n",
    "        features=Features(entities=EntitiesOptions(), \n",
    "                          keywords=KeywordsOptions()))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Augumented Classification\n",
    "\n",
    "Custom classification utlity fucntions for augumenting the results of Watson NLU API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "    \"\"\" Tag the text matching keywords.\n",
    "    \"\"\"\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "def regex_tagging(tag,regex,text):\n",
    "    \"\"\" Tag the text matching REGEX.\n",
    "    \"\"\"    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "def chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "    \n",
    "def augument_NLUResponse(responsejson,updateType,text,tag):\n",
    "    \"\"\" Update the NLU response JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n",
    "            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['entities']):\n",
    "            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1})        \n",
    "    \n",
    "\n",
    "def classify_text(text, config):\n",
    "    \"\"\" Perform augumented classification of the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = analyze_using_NLU(text)\n",
    "    responsejson = response\n",
    "    \n",
    "    sentenceList = split_sentences(text)\n",
    "    \n",
    "    tokens = split_into_tokens(text)\n",
    "    \n",
    "    postags = POS_tagging(tokens)\n",
    "    \n",
    "    configjson = json.loads(config)\n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        print('Stage - Performing ' + stages['name']+':')\n",
    "        for steps in stages['steps']:\n",
    "            print('    Step - ' + steps['type']+':')\n",
    "            if (steps['type'] == 'keywords'):\n",
    "                for keyword in steps['keywords']:\n",
    "                    for word in sentenceList:\n",
    "                        wordtag = keyword_tagging(keyword['tag'],keyword['text'],word)\n",
    "                        if(wordtag != 'UNKNOWN'):\n",
    "                            print('      '+keyword['tag']+':'+wordtag)\n",
    "                            augument_NLUResponse(responsejson,'entities',wordtag,keyword['tag'])\n",
    "            elif(steps['type'] == 'd_regex'):\n",
    "                for regex in steps['d_regex']:\n",
    "                    for word in sentenceList:\n",
    "                        regextags = regex_tagging(regex['tag'],regex['pattern'],word)\n",
    "                        if (len(regextags)>0):\n",
    "                            for words in regextags:\n",
    "                                print('      '+regex['tag']+':'+words)\n",
    "                                augument_NLUResponse(responsejson,'entities',words,regex['tag'])\n",
    "            elif(steps['type'] == 'chunking'):\n",
    "                for chunk in steps['chunk']:\n",
    "                    chunktags = chunk_tagging(chunk['tag'],chunk['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            print('      '+chunk['tag']+':'+words)\n",
    "                            augument_NLUResponse(responsejson,'entities',words,chunk['tag'])\n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    return responsejson\n",
    "\n",
    "def replace_unicode_strings(response):\n",
    "    \"\"\" Convert dict with unicode strings to strings.\n",
    "    \"\"\"\n",
    "    if isinstance(response, dict):\n",
    "        return {replace_unicode_strings(key): replace_unicode_strings(value) for key, value in response.iteritems()}\n",
    "    elif isinstance(response, list):\n",
    "        return [replace_unicode_strings(element) for element in response]\n",
    "    elif isinstance(response, unicode):\n",
    "        return response.encode('utf-8')\n",
    "    else:\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Persistence and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 Configure Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth_url = credentials_1['auth_url']+\"/v3\"\n",
    "container = credentials_1[\"container\"]\n",
    "\n",
    "IBM_Objectstorage_Connection = swiftclient.Connection(\n",
    "    key=credentials_1['password'], authurl=auth_url, auth_version='3', os_options={\n",
    "        \"project_id\": credentials_1['project_id'], \"user_id\": credentials_1['user_id'], \"region_name\": credentials_1['region']})\n",
    "\n",
    "def create_container(container_name):\n",
    "    \"\"\" Create a container on Object Storage.\n",
    "    \"\"\"\n",
    "    x = IBM_Objectstorage_Connection.put_container(container_name)\n",
    "    return x\n",
    "\n",
    "def put_object(container_name, fname, contents, content_type):\n",
    "    \"\"\" Write contents to Object Storage.\n",
    "    \"\"\"\n",
    "    x = IBM_Objectstorage_Connection.put_object(\n",
    "        container_name,\n",
    "        fname,\n",
    "        contents,\n",
    "        content_type)\n",
    "    return x\n",
    "\n",
    "def get_object(container_name, fname):\n",
    "    \"\"\" Retrieve contents from Object Storage.\n",
    "    \"\"\"\n",
    "    Object_Store_file_details = IBM_Objectstorage_Connection.get_object(\n",
    "        container_name, fname)\n",
    "    return Object_Store_file_details[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classify text\n",
    "Read the data file for classification from Object Store<br>\n",
    "Read the configuration file for augumented classification from Object Store.<br>\n",
    "Persist the classification results as JSON file in object store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Using the configuration ##\n",
      "{\n",
      "  \"configuration\": {\n",
      "    \"classification\": {\n",
      "      \"stages\": [\n",
      "        {\n",
      "          \"name\": \"Base Tagging\",\n",
      "          \"steps\": [\n",
      "            {\n",
      "              \"type\": \"keywords\",\n",
      "              \"keywords\": [\n",
      "                {\n",
      "                  \"tag\": \"Passion\",\n",
      "                  \"text\": \"Science\"\n",
      "                },\n",
      "                {\n",
      "                  \"tag\": \"Subjects\",\n",
      "                  \"text\": \"cosmology\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"d_regex\",\n",
      "              \"d_regex\": [\n",
      "                {\n",
      "                  \"tag\": \"Date\",\n",
      "                  \"pattern\": \"(\\\\d+/\\\\d+/\\\\d+)\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"d_regex\",\n",
      "              \"d_regex\": [\n",
      "                {\n",
      "                  \"tag\": \"Email\",\n",
      "                  \"pattern\": \"\\\\b[\\\\w.-]+?@\\\\w+?\\\\.\\\\w+?\\\\b\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"d_regex\",\n",
      "              \"d_regex\": [\n",
      "                {\n",
      "                  \"tag\": \"PhoneNumber\",\n",
      "                  \"pattern\": \"[0-9]{10}\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"chunking\",\n",
      "              \"chunk\": [\n",
      "                {\n",
      "                  \"tag\": \"NP\",\n",
      "                  \"pattern\": \"NP:{<DT>?<JJ>*<NN>}\"\n",
      "                },\n",
      "                {\n",
      "                  \"tag\": \"NAME\",\n",
      "                  \"pattern\": \"NAME:{<NNP>+}\"\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"Domain Tagging\",\n",
      "          \"steps\": [\n",
      "            {\n",
      "              \"type\": \"d_regex\",\n",
      "              \"d_regex\": [\n",
      "                {\n",
      "                  \"tag\": \"Year\",\n",
      "                  \"pattern\": \"[0-9]{4}\"\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the text from Object Storage\n",
    "text = get_object(container, sampleTextFileName)\n",
    "\n",
    "# Load the json configuration from Object Storage\n",
    "config = get_object(container, sampleConfigFileName)\n",
    "\n",
    "# Print the json configuration\n",
    "print(\"## Using the configuration ##\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage - Performing Base Tagging:\n",
      "    Step - keywords:\n",
      "      Passion:science\n",
      "      Passion:science\n",
      "      Subjects:cosmology\n",
      "      Subjects:cosmology\n",
      "    Step - d_regex:\n",
      "      Date:01/08/1942\n",
      "    Step - d_regex:\n",
      "    Step - d_regex:\n",
      "      PhoneNumber:1112223333\n",
      "    Step - chunking:\n",
      "      NP: an early age\n",
      "      NP: a passion\n",
      "      NP: science\n",
      "      NP: the sky\n",
      "      NP: age\n",
      "      NP: cosmology\n",
      "      NP: amyotrophic lateral sclerosis\n",
      "      NP: illness\n",
      "      NP: work\n",
      "      NP: cosmology\n",
      "      NP: science\n",
      "      NP: everyone\n",
      "      NP: phone\n",
      "      NP: email\n",
      "      NP: yahoo.com\n",
      "      NAME: Stephen Hawking\n",
      "      NAME: Oxford\n",
      "      NAME: England\n",
      "      NAME: Hawking\n",
      "      NAME: University\n",
      "      NAME: Cambridge\n",
      "      NAME: Stephen Hawking\n",
      "      NAME: @\n",
      "Stage - Performing Domain Tagging:\n",
      "    Step - d_regex:\n",
      "      Year:1942\n",
      "      Year:1112\n",
      "      Year:2233\n"
     ]
    }
   ],
   "source": [
    "# Classify the text\n",
    "response = classify_text(text, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ Text Classification ~~\n",
      "{'usage': {'text_characters': 502, 'features': 2, 'text_units': 1}, 'keywords': [{'relevance': 0.96866, 'text': 'Stephen Hawking'}, {'relevance': 0.884134, 'text': 'amyotrophic lateral sclerosis'}, {'relevance': 0.718936, 'text': 'debilitating illness'}, {'relevance': 0.66862, 'text': 'early age'}, {'relevance': 0.575803, 'text': 'cosmology'}, {'relevance': 0.513458, 'text': 'science'}, {'relevance': 0.492845, 'text': 'yahoo.com'}, {'relevance': 0.475153, 'text': 'passion'}, {'relevance': 0.464197, 'text': 'Oxford'}, {'relevance': 0.464044, 'text': 'England'}, {'relevance': 0.45856, 'text': 'sky'}, {'relevance': 0.4566, 'text': 'University'}, {'relevance': 0.456461, 'text': 'Cambridge'}, {'relevance': 0.453992, 'text': 'work'}, {'relevance': 0.45387, 'text': 'phone'}, {'relevance': 0.453855, 'text': 'physics'}], 'language': 'en', 'entities': [{'relevance': 0.846941, 'text': 'Stephen Hawking', 'disambiguation': {'subtype': ['Academic', 'Astronomer', 'AwardNominee', 'AwardWinner', 'BoardMember', 'Scientist', 'FilmActor', 'FilmWriter', 'TVActor'], 'name': 'Stephen Hawking', 'dbpedia_resource': 'http://dbpedia.org/resource/Stephen_Hawking'}, 'type': 'Person', 'count': 5}, {'relevance': 0.202166, 'text': 'University of Cambridge', 'disambiguation': {'subtype': ['Location', 'CollegeUniversity', 'ProcessorManufacturer', 'University'], 'name': 'University of Cambridge', 'dbpedia_resource': 'http://dbpedia.org/resource/University_of_Cambridge'}, 'type': 'Organization', 'count': 1}, {'relevance': 0.200592, 'text': 'Oxford', 'disambiguation': {'subtype': ['AdministrativeDivision', 'PlaceWithNeighborhoods', 'City'], 'name': 'Oxford', 'dbpedia_resource': 'http://dbpedia.org/resource/Oxford'}, 'type': 'Location', 'count': 1}, {'relevance': 0.180876, 'text': 'England', 'disambiguation': {'subtype': ['PoliticalDistrict', 'AdministrativeDivision', 'GovernmentalJurisdiction', 'Country'], 'name': 'England', 'dbpedia_resource': 'http://dbpedia.org/resource/England'}, 'type': 'Location', 'count': 1}, {'relevance': 0.180876, 'text': 'shawking@yahoo.com', 'type': 'EmailAddress', 'count': 1}, {'relevance': 0.5, 'text': 'science', 'type': 'Passion', 'count': 1}, {'relevance': 0.5, 'text': 'cosmology', 'type': 'Subjects', 'count': 1}, {'relevance': 0.5, 'text': '01/08/1942', 'type': 'Date', 'count': 1}, {'relevance': 0.5, 'text': '1112223333', 'type': 'PhoneNumber', 'count': 1}, {'relevance': 0.5, 'text': ' an early age', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' a passion', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' science', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' the sky', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' age', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' cosmology', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' amyotrophic lateral sclerosis', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' illness', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' work', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' everyone', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' phone', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' email', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' yahoo.com', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' Stephen Hawking', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Oxford', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' England', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Hawking', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' University', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Cambridge', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' @', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': '1942', 'type': 'Year', 'count': 1}, {'relevance': 0.5, 'text': '1112', 'type': 'Year', 'count': 1}, {'relevance': 0.5, 'text': '2233', 'type': 'Year', 'count': 1}]}\n"
     ]
    }
   ],
   "source": [
    "# replace unicode strings and convert dict to str for storage\n",
    "response = str(replace_unicode_strings(response))\n",
    "print(\"~~ Text Classification ~~\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'usage': {'text_characters': 502, 'features': 2, 'text_units': 1}, 'keywords': [{'relevance': 0.96866, 'text': 'Stephen Hawking'}, {'relevance': 0.884134, 'text': 'amyotrophic lateral sclerosis'}, {'relevance': 0.718936, 'text': 'debilitating illness'}, {'relevance': 0.66862, 'text': 'early age'}, {'relevance': 0.575803, 'text': 'cosmology'}, {'relevance': 0.513458, 'text': 'science'}, {'relevance': 0.492845, 'text': 'yahoo.com'}, {'relevance': 0.475153, 'text': 'passion'}, {'relevance': 0.464197, 'text': 'Oxford'}, {'relevance': 0.464044, 'text': 'England'}, {'relevance': 0.45856, 'text': 'sky'}, {'relevance': 0.4566, 'text': 'University'}, {'relevance': 0.456461, 'text': 'Cambridge'}, {'relevance': 0.453992, 'text': 'work'}, {'relevance': 0.45387, 'text': 'phone'}, {'relevance': 0.453855, 'text': 'physics'}], 'language': 'en', 'entities': [{'relevance': 0.846941, 'text': 'Stephen Hawking', 'disambiguation': {'subtype': ['Academic', 'Astronomer', 'AwardNominee', 'AwardWinner', 'BoardMember', 'Scientist', 'FilmActor', 'FilmWriter', 'TVActor'], 'name': 'Stephen Hawking', 'dbpedia_resource': 'http://dbpedia.org/resource/Stephen_Hawking'}, 'type': 'Person', 'count': 5}, {'relevance': 0.202166, 'text': 'University of Cambridge', 'disambiguation': {'subtype': ['Location', 'CollegeUniversity', 'ProcessorManufacturer', 'University'], 'name': 'University of Cambridge', 'dbpedia_resource': 'http://dbpedia.org/resource/University_of_Cambridge'}, 'type': 'Organization', 'count': 1}, {'relevance': 0.200592, 'text': 'Oxford', 'disambiguation': {'subtype': ['AdministrativeDivision', 'PlaceWithNeighborhoods', 'City'], 'name': 'Oxford', 'dbpedia_resource': 'http://dbpedia.org/resource/Oxford'}, 'type': 'Location', 'count': 1}, {'relevance': 0.180876, 'text': 'England', 'disambiguation': {'subtype': ['PoliticalDistrict', 'AdministrativeDivision', 'GovernmentalJurisdiction', 'Country'], 'name': 'England', 'dbpedia_resource': 'http://dbpedia.org/resource/England'}, 'type': 'Location', 'count': 1}, {'relevance': 0.180876, 'text': 'shawking@yahoo.com', 'type': 'EmailAddress', 'count': 1}, {'relevance': 0.5, 'text': 'science', 'type': 'Passion', 'count': 1}, {'relevance': 0.5, 'text': 'cosmology', 'type': 'Subjects', 'count': 1}, {'relevance': 0.5, 'text': '01/08/1942', 'type': 'Date', 'count': 1}, {'relevance': 0.5, 'text': '1112223333', 'type': 'PhoneNumber', 'count': 1}, {'relevance': 0.5, 'text': ' an early age', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' a passion', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' science', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' the sky', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' age', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' cosmology', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' amyotrophic lateral sclerosis', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' illness', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' work', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' everyone', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' phone', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' email', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' yahoo.com', 'type': 'NP', 'count': 1}, {'relevance': 0.5, 'text': ' Stephen Hawking', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Oxford', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' England', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Hawking', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' University', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' Cambridge', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': ' @', 'type': 'NAME', 'count': 1}, {'relevance': 0.5, 'text': '1942', 'type': 'Year', 'count': 1}, {'relevance': 0.5, 'text': '1112', 'type': 'Year', 'count': 1}, {'relevance': 0.5, 'text': '2233', 'type': 'Year', 'count': 1}]}\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the classification response in Object Storage\n",
    "put_object(container, \"sample_text_classification.txt\", response, \"text\")\n",
    "\n",
    "# Retrieve classification response from Object Storage\n",
    "get_object(container, \"sample_text_classification.txt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6 (Unsupported)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
